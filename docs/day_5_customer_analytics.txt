========================================
DAY 5 SPRINT ‚Äì Customer Analytics ETL
========================================

üéØ OBJECTIVE
- Load ecommerce_sales.csv into staging (Postgres)
- Normalize into DWH star schema (dim_date, dim_customer, dim_product, fact_sales)
- Export portfolio artifacts for BI-style insights
- Transition tooling to Visual Studio Code

üìù TASKS
1. Set up VS Code for repo with Python venv & WSL Postgres.
2. Built day5_autoload_csv.py (with --encoding option).
   - Input: datasets/ecommerce_sales.csv (541,909 rows).
   - Schema: stage.raw_ecommerce
   - ‚úÖ Loaded successfully (row count verified).
3. Created DWH schema (dwh.*) with 4 star-model tables.
4. Populated dims & fact using day5_populate_dim_fact.sql:
   - dim_customer: 4,373
   - dim_date: 305
   - dim_product: 4,071
   - fact_sales: 1,083,818
5. Exported summary artifacts:
   - artifacts/day5/sales_by_country_summary.csv
   - artifacts/day5/top_products_by_revenue.csv
6. Verified ETL counts and confirmed data pipeline flow.

üíª COMMANDS / CODE
- python scripts/sql/day5_autoload_csv.py --csv datasets/ecommerce_sales.csv --schema stage --table raw_ecommerce --replace --encoding LATIN1
- psql -d "$PGDATABASE" -f scripts/sql/day5_create_tables.sql
- psql -d "$PGDATABASE" -f scripts/sql/day5_populate_dim_fact.sql
- bash scripts/sql/day5_export_artifacts.sh

üìä RESULTS
- Staging row count: 541,909
- DWH populated with non-zero dimensions & fact rows
- Generated CSV summaries (sales by country, top products)

üñºÔ∏è SCREENSHOTS
- VS Code with repo & terminal
- psql row counts for staging and DWH
- artifact previews (head of CSV files)

‚úÖ OUTCOME
Day 5 completed. Repo now demonstrates:
- Hands-on ETL (CSV ‚Üí staging ‚Üí star schema ‚Üí fact/dims)
- Data modeling skills (dim/fact design)
- Analytics outputs for portfolio
